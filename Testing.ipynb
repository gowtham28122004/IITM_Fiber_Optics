{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbvCe67m3sSE",
        "outputId": "df49a0a7-8d57-47c2-8fc0-2a9baae5e223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score, roc_curve, auc\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "4qMTVLvBGmpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SEQUENCE_LENGTH = 25000\n",
        "CLASS_NAMES = [\"Bike Throttle\", \"Jackhammer\", \"Jumping\", \"Walking\"]\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = f\"/content/drive/MyDrive/das_testing_results_{timestamp}\""
      ],
      "metadata": {
        "id": "3pyHf-BaGoh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create results directory if it doesn't exist\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "X-Xh_SXwGqjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_and_model():\n",
        "    \"\"\"Load and prepare data and model\"\"\"\n",
        "    # Paths to your data files - update these paths\n",
        "    x_train_path = '/content/drive/MyDrive/X_train.npy'\n",
        "    y_train_path = '/content/drive/MyDrive/y_train.npy'\n",
        "    x_test_path = '/content/drive/MyDrive/X_test.npy'\n",
        "    y_test_path = '/content/drive/MyDrive/y_test.npy'\n",
        "    model_path = '/content/drive/MyDrive/das_model_output_20250304_032635/model_3/model_3.keras'\n",
        "\n",
        "    # Load and downsample data\n",
        "    X_train, y_train, X_test, y_test, new_seq_length = load_data(\n",
        "        x_train_path, y_train_path, x_test_path, y_test_path, downsample_factor=5\n",
        "    )\n",
        "\n",
        "    # Print original shapes for verification\n",
        "    print(\"Loading data and model...\")\n",
        "    print(f\"Original X_train shape: (64000, 25000)\")\n",
        "    print(f\"Original y_train shape: {y_train.shape}\")\n",
        "    print(f\"Original X_test shape: (16000, 25000)\")\n",
        "    print(f\"Original y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Load model with custom object scope to handle any custom layers\n",
        "    custom_objects = {}  # Add any custom layers/metrics here if needed\n",
        "\n",
        "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "        try:\n",
        "            model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "            # Initialize the model with a small batch to fix shape issues\n",
        "            _ = model(X_test[:2], training=False)\n",
        "        except Exception as e:\n",
        "            print(f\"Standard model loading failed: {str(e)}\")\n",
        "            print(\"Trying alternative approach...\")\n",
        "\n",
        "            # Create model architecture with explicit input shape\n",
        "            # Replace this with your actual model architecture\n",
        "            inputs = tf.keras.Input(shape=(new_seq_length, 1))\n",
        "            x = tf.keras.layers.Conv1D(32, 3, activation='relu')(inputs)\n",
        "            x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "            x = tf.keras.layers.Flatten()(x)\n",
        "            outputs = tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')(x)\n",
        "            model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            # Load weights\n",
        "            model.load_weights(model_path, by_name=True)\n",
        "\n",
        "            # Initialize the model with a small batch\n",
        "            _ = model(X_test[:2], training=False)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, model"
      ],
      "metadata": {
        "id": "ZK0PbzuIGxhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(x_train_path, y_train_path, x_test_path, y_test_path, downsample_factor=5):\n",
        "    \"\"\"Load data with memory mapping and downsampling\"\"\"\n",
        "    print(\"Loading training data...\")\n",
        "    X_train = np.load(x_train_path, mmap_mode='r')\n",
        "    y_train = np.load(y_train_path)\n",
        "\n",
        "    print(\"Loading test data...\")\n",
        "    X_test = np.load(x_test_path, mmap_mode='r')\n",
        "    y_test = np.load(y_test_path)\n",
        "\n",
        "    # Calculate new sequence length\n",
        "    new_seq_length = SEQUENCE_LENGTH // downsample_factor\n",
        "\n",
        "    # Process in batches for memory efficiency\n",
        "    print(\"Downsampling data...\")\n",
        "    X_train_shape = (X_train.shape[0], new_seq_length, 1)\n",
        "    X_test_shape = (X_test.shape[0], new_seq_length, 1)\n",
        "\n",
        "    X_train_downsampled = np.zeros(X_train_shape, dtype=np.float32)\n",
        "    X_test_downsampled = np.zeros(X_test_shape, dtype=np.float32)\n",
        "\n",
        "    batch_size = 100  # Process in small batches to save memory\n",
        "\n",
        "    # Downsample training data in batches\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        end_idx = min(i + batch_size, X_train.shape[0])\n",
        "        batch = X_train[i:end_idx].reshape(-1, SEQUENCE_LENGTH, 1)\n",
        "        for j in range(end_idx - i):\n",
        "            X_train_downsampled[i+j, :, 0] = batch[j, ::downsample_factor, 0][:new_seq_length]\n",
        "\n",
        "    # Downsample test data in batches\n",
        "    for i in range(0, X_test.shape[0], batch_size):\n",
        "        end_idx = min(i + batch_size, X_test.shape[0])\n",
        "        batch = X_test[i:end_idx].reshape(-1, SEQUENCE_LENGTH, 1)\n",
        "        for j in range(end_idx - i):\n",
        "            X_test_downsampled[i+j, :, 0] = batch[j, ::downsample_factor, 0][:new_seq_length]\n",
        "\n",
        "    print(f\"Downsampled X_train shape: {X_train_downsampled.shape}\")\n",
        "    print(f\"y_train shape: {y_train.shape}\")\n",
        "    print(f\"Downsampled X_test shape: {X_test_downsampled.shape}\")\n",
        "    print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "    return X_train_downsampled, y_train, X_test_downsampled, y_test, new_seq_length"
      ],
      "metadata": {
        "id": "1TDETdY7G1Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset_integrity(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Check dataset integrity for various issues\"\"\"\n",
        "    print(\"\\n========== DATASET INTEGRITY CHECK ==========\")\n",
        "\n",
        "    # Check for NaN and infinity values\n",
        "    nan_train_X = np.isnan(X_train).sum()\n",
        "    nan_train_y = np.isnan(y_train).sum()\n",
        "    nan_test_X = np.isnan(X_test).sum()\n",
        "    nan_test_y = np.isnan(y_test).sum()\n",
        "\n",
        "    inf_train_X = np.isinf(X_train).sum()\n",
        "    inf_test_X = np.isinf(X_test).sum()\n",
        "\n",
        "    print(f\"NaN values in X_train: {nan_train_X}\")\n",
        "    print(f\"NaN values in y_train: {nan_train_y}\")\n",
        "    print(f\"NaN values in X_test: {nan_test_X}\")\n",
        "    print(f\"NaN values in y_test: {nan_test_y}\")\n",
        "    print(f\"Infinity values in X_train: {inf_train_X}\")\n",
        "    print(f\"Infinity values in X_test: {inf_test_X}\")\n",
        "\n",
        "    # Check for duplicates (sample a subset for efficiency)\n",
        "    sample_size = min(10000, X_train.shape[0])\n",
        "    sample_indices = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
        "    X_sample = X_train[sample_indices].reshape(sample_size, -1)\n",
        "\n",
        "    # Check for duplicate rows\n",
        "    unique_rows = np.unique(X_sample, axis=0)\n",
        "    if len(unique_rows) < sample_size:\n",
        "        duplicates = sample_size - len(unique_rows)\n",
        "        print(f\"Found {duplicates} duplicate rows in the sampled subset of training data\")\n",
        "    else:\n",
        "        print(\"No duplicate rows found in the sampled subset of training data\")\n",
        "\n",
        "    # Check data range\n",
        "    x_train_min = X_train.min()\n",
        "    x_train_max = X_train.max()\n",
        "    x_test_min = X_test.min()\n",
        "    x_test_max = X_test.max()\n",
        "\n",
        "    print(f\"X_train min: {x_train_min}, max: {x_train_max}\")\n",
        "    print(f\"X_test min: {x_test_min}, max: {x_test_max}\")\n",
        "\n",
        "    # Save results\n",
        "    integrity_report = {\n",
        "        \"nan_values\": {\n",
        "            \"X_train\": int(nan_train_X),\n",
        "            \"y_train\": int(nan_train_y),\n",
        "            \"X_test\": int(nan_test_X),\n",
        "            \"y_test\": int(nan_test_y)\n",
        "        },\n",
        "        \"infinity_values\": {\n",
        "            \"X_train\": int(inf_train_X),\n",
        "            \"X_test\": int(inf_test_X)\n",
        "        },\n",
        "        \"duplicates_check\": {\n",
        "            \"sample_size\": sample_size,\n",
        "            \"unique_rows\": len(unique_rows),\n",
        "            \"has_duplicates\": len(unique_rows) < sample_size\n",
        "        },\n",
        "        \"data_range\": {\n",
        "            \"X_train_min\": float(x_train_min),\n",
        "            \"X_train_max\": float(x_train_max),\n",
        "            \"X_test_min\": float(x_test_min),\n",
        "            \"X_test_max\": float(x_test_max)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return integrity_report"
      ],
      "metadata": {
        "id": "npYP0lqVG43-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_class_distribution(y_train, y_test):\n",
        "    \"\"\"Analyze class distribution and imbalance\"\"\"\n",
        "    print(\"\\n========== CLASS DISTRIBUTION ANALYSIS ==========\")\n",
        "\n",
        "    # Convert one-hot encoded labels to class indices\n",
        "    y_train_classes = np.argmax(y_train, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Count occurrences of each class\n",
        "    train_counts = np.bincount(y_train_classes)\n",
        "    test_counts = np.bincount(y_test_classes)\n",
        "\n",
        "    # Calculate total samples\n",
        "    train_total = len(y_train_classes)\n",
        "    test_total = len(y_test_classes)\n",
        "\n",
        "    # Calculate percentages\n",
        "    train_percentages = (train_counts / train_total) * 100\n",
        "    test_percentages = (test_counts / test_total) * 100\n",
        "\n",
        "    # Print class distribution\n",
        "    print(\"Training set class distribution:\")\n",
        "    for i, class_name in enumerate(CLASS_NAMES):\n",
        "        print(f\"{class_name}: {train_counts[i]} samples ({train_percentages[i]:.2f}%)\")\n",
        "\n",
        "    print(\"\\nTest set class distribution:\")\n",
        "    for i, class_name in enumerate(CLASS_NAMES):\n",
        "        print(f\"{class_name}: {test_counts[i]} samples ({test_percentages[i]:.2f}%)\")\n",
        "\n",
        "    # Calculate imbalance (difference between most and least represented classes)\n",
        "    train_imbalance = np.max(train_percentages) - np.min(train_percentages)\n",
        "    test_imbalance = np.max(test_percentages) - np.min(test_percentages)\n",
        "\n",
        "    print(f\"\\nTraining set imbalance: {train_imbalance:.2f}%\")\n",
        "    print(f\"Test set imbalance: {test_imbalance:.2f}%\")\n",
        "\n",
        "    # Plot class distribution\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    ax1.bar(CLASS_NAMES, train_counts)\n",
        "    ax1.set_title('Training Set Class Distribution')\n",
        "    ax1.set_ylabel('Count')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    ax2.bar(CLASS_NAMES, test_counts)\n",
        "    ax2.set_title('Test Set Class Distribution')\n",
        "    ax2.set_ylabel('Count')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/class_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save results\n",
        "    distribution_report = {\n",
        "        \"training_set\": {\n",
        "            \"counts\": train_counts.tolist(),\n",
        "            \"percentages\": train_percentages.tolist(),\n",
        "            \"imbalance\": float(train_imbalance)\n",
        "        },\n",
        "        \"test_set\": {\n",
        "            \"counts\": test_counts.tolist(),\n",
        "            \"percentages\": test_percentages.tolist(),\n",
        "            \"imbalance\": float(test_imbalance)\n",
        "        },\n",
        "        \"class_names\": CLASS_NAMES\n",
        "    }\n",
        "\n",
        "    return distribution_report"
      ],
      "metadata": {
        "id": "bo2uNMRWG9YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_unknown_class(X_test, model):\n",
        "    \"\"\"Simulate unknown class detection with confidence thresholds\"\"\"\n",
        "    print(\"\\n========== UNKNOWN CLASS SIMULATION ==========\")\n",
        "\n",
        "    # Generate random signals to simulate unknown class\n",
        "    np.random.seed(42)\n",
        "    n_samples = 200\n",
        "    signal_length = X_test.shape[1]\n",
        "    channels = X_test.shape[2]\n",
        "\n",
        "    # Create random signals with same statistics as original data\n",
        "    mean = np.mean(X_test)\n",
        "    std = np.std(X_test)\n",
        "    unknown_signals = np.random.normal(mean, std, (n_samples, signal_length, channels))\n",
        "\n",
        "    # Predict on unknown signals\n",
        "    y_pred_prob = model.predict(unknown_signals)\n",
        "\n",
        "    # Use max probability as confidence score\n",
        "    confidence_scores = np.max(y_pred_prob, axis=1)\n",
        "\n",
        "    # Apply confidence threshold (0.7 is example threshold)\n",
        "    threshold = 0.7\n",
        "    low_confidence = confidence_scores < threshold\n",
        "    num_unknown_detected = np.sum(low_confidence)\n",
        "\n",
        "    print(f\"Generated {n_samples} unknown signals\")\n",
        "    print(f\"Signals classified as 'unknown' (confidence < {threshold}): {num_unknown_detected} ({num_unknown_detected/n_samples*100:.2f}%)\")\n",
        "\n",
        "    # Plot confidence distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(confidence_scores, bins=20)\n",
        "    plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold})')\n",
        "    plt.xlabel('Confidence Score')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Confidence Distribution for Unknown Signals')\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/unknown_class_confidence.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save results\n",
        "    unknown_report = {\n",
        "        \"n_samples\": n_samples,\n",
        "        \"threshold\": threshold,\n",
        "        \"unknown_detected\": int(num_unknown_detected),\n",
        "        \"detection_rate\": float(num_unknown_detected/n_samples),\n",
        "        \"confidence_scores\": {\n",
        "            \"min\": float(np.min(confidence_scores)),\n",
        "            \"max\": float(np.max(confidence_scores)),\n",
        "            \"mean\": float(np.mean(confidence_scores)),\n",
        "            \"std\": float(np.std(confidence_scores)),\n",
        "            \"quantiles\": [float(np.quantile(confidence_scores, q)) for q in [0.25, 0.5, 0.75]]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return unknown_report"
      ],
      "metadata": {
        "id": "ntKtINxJHA02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_confusion_matrix(X_test, y_test, model):\n",
        "    \"\"\"Analyze confusion matrix for the model predictions\"\"\"\n",
        "    print(\"\\n========== CONFUSION MATRIX ANALYSIS ==========\")\n",
        "\n",
        "    # Get predictions using model.predict()\n",
        "    try:\n",
        "        # Predict on test data\n",
        "        y_pred_prob = model.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    except:\n",
        "        print(\"Error using model.predict(). Trying alternative approach...\")\n",
        "        # Try using model.__call__ with TensorFlow tensors\n",
        "        y_pred_prob = []\n",
        "        batch_size = 64  # Smaller batch size to avoid memory issues\n",
        "\n",
        "        for i in range(0, len(X_test), batch_size):\n",
        "            batch = X_test[i:i+batch_size]\n",
        "            batch_tensor = tf.convert_to_tensor(batch)\n",
        "            pred = model(batch_tensor, training=False)\n",
        "            y_pred_prob.append(pred.numpy())\n",
        "\n",
        "        y_pred_prob = np.vstack(y_pred_prob)\n",
        "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    for i, class_name in enumerate(CLASS_NAMES):\n",
        "        print(f\"\\nClass: {class_name}\")\n",
        "        print(f\"Precision: {precision[i]:.4f}\")\n",
        "        print(f\"Recall: {recall[i]:.4f}\")\n",
        "        print(f\"F1 Score: {f1[i]:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save results\n",
        "    cm_results = {\n",
        "        \"confusion_matrix\": cm.tolist(),\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"precision\": precision.tolist(),\n",
        "        \"recall\": recall.tolist(),\n",
        "        \"f1_score\": f1.tolist(),\n",
        "        \"class_names\": CLASS_NAMES\n",
        "    }\n",
        "\n",
        "    with open(f\"{RESULTS_DIR}/confusion_matrix_results.json\", 'w') as f:\n",
        "        json.dump(cm_results, f, indent=4)\n",
        "\n",
        "    return cm_results"
      ],
      "metadata": {
        "id": "jm6iyYAZHDuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(X_test, y_test, model):\n",
        "    \"\"\"Evaluate various metrics including ROC curves\"\"\"\n",
        "    print(\"\\n========== METRICS EVALUATION ==========\")\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate ROC curves and AUC for each class\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "\n",
        "    for i, class_name in enumerate(CLASS_NAMES):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        print(f\"ROC AUC for {class_name}: {roc_auc[i]:.4f}\")\n",
        "\n",
        "    # Calculate macro-averaged ROC curve and ROC area\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(CLASS_NAMES))]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "\n",
        "    for i in range(len(CLASS_NAMES)):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "    mean_tpr /= len(CLASS_NAMES)\n",
        "    macro_roc_auc = auc(all_fpr, mean_tpr)\n",
        "    print(f\"Macro-averaged ROC AUC: {macro_roc_auc:.4f}\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    plt.plot(all_fpr, mean_tpr,\n",
        "             label=f'Macro-average ROC (AUC = {macro_roc_auc:.4f})',\n",
        "             color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "    for i, class_name in enumerate(CLASS_NAMES):\n",
        "        plt.plot(fpr[i], tpr[i],\n",
        "                 label=f'ROC for {class_name} (AUC = {roc_auc[i]:.4f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(f\"{RESULTS_DIR}/roc_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save results\n",
        "    metrics_results = {\n",
        "        \"roc_auc_per_class\": {class_name: float(roc_auc[i]) for i, class_name in enumerate(CLASS_NAMES)},\n",
        "        \"macro_roc_auc\": float(macro_roc_auc)\n",
        "    }\n",
        "\n",
        "    return metrics_results"
      ],
      "metadata": {
        "id": "QtxGjZYaHYqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_cross_validation(X_train, y_train, model):\n",
        "    \"\"\"Perform k-fold cross validation\"\"\"\n",
        "    print(\"\\n========== CROSS VALIDATION ==========\")\n",
        "\n",
        "    # Use 5-fold cross validation\n",
        "    k_folds = 5\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    # We'll use a smaller subset for cross-validation to save time\n",
        "    sample_size = min(10000, X_train.shape[0])\n",
        "    indices = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
        "    X_sample = X_train[indices]\n",
        "    y_sample = y_train[indices]\n",
        "\n",
        "    # Store metrics for each fold\n",
        "    accuracy_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    print(f\"Performing {k_folds}-fold cross-validation on {sample_size} samples...\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_sample)):\n",
        "        print(f\"Fold {fold+1}/{k_folds}\")\n",
        "\n",
        "        # Split data\n",
        "        X_fold_train, X_fold_val = X_sample[train_idx], X_sample[val_idx]\n",
        "        y_fold_train, y_fold_val = y_sample[train_idx], y_sample[val_idx]\n",
        "\n",
        "        # Skip the model retraining part for time efficiency\n",
        "        # Just evaluate on the validation fold using the original model\n",
        "        y_fold_pred = np.argmax(model.predict(X_fold_val), axis=1)\n",
        "        y_fold_true = np.argmax(y_fold_val, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_fold_true, y_fold_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            y_fold_true, y_fold_pred, average='macro'\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        accuracy_scores.append(accuracy)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    print(\"\\nCross-validation results:\")\n",
        "    print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
        "    print(f\"Mean Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
        "    print(f\"Mean Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
        "    print(f\"Mean F1 Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "    # Save results\n",
        "    cv_results = {\n",
        "        \"k_folds\": k_folds,\n",
        "        \"sample_size\": sample_size,\n",
        "        \"accuracy\": {\n",
        "            \"values\": [float(x) for x in accuracy_scores],\n",
        "            \"mean\": float(np.mean(accuracy_scores)),\n",
        "            \"std\": float(np.std(accuracy_scores))\n",
        "        },\n",
        "        \"precision\": {\n",
        "            \"values\": [float(x) for x in precision_scores],\n",
        "            \"mean\": float(np.mean(precision_scores)),\n",
        "            \"std\": float(np.std(precision_scores))\n",
        "        },\n",
        "        \"recall\": {\n",
        "            \"values\": [float(x) for x in recall_scores],\n",
        "            \"mean\": float(np.mean(recall_scores)),\n",
        "            \"std\": float(np.std(recall_scores))\n",
        "        },\n",
        "        \"f1_score\": {\n",
        "            \"values\": [float(x) for x in f1_scores],\n",
        "            \"mean\": float(np.mean(f1_scores)),\n",
        "            \"std\": float(np.std(f1_scores))\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return cv_results"
      ],
      "metadata": {
        "id": "RFkrb1SPHcA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_noise_resilience(X_test, y_test, model):\n",
        "    \"\"\"Test model resilience to different noise levels\"\"\"\n",
        "    print(\"\\n========== NOISE RESILIENCE TESTING ==========\")\n",
        "\n",
        "    # Define noise levels to test\n",
        "    noise_levels = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
        "\n",
        "    # Store results for each noise level\n",
        "    accuracy_results = []\n",
        "\n",
        "    # Get baseline accuracy\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    baseline_accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Baseline accuracy (no noise): {baseline_accuracy:.4f}\")\n",
        "\n",
        "    # Test each noise level\n",
        "    for noise_level in noise_levels:\n",
        "        print(f\"Testing noise level: {noise_level}\")\n",
        "\n",
        "        # Create noisy data\n",
        "        noise = np.random.normal(0, noise_level, X_test.shape)\n",
        "        X_test_noisy = X_test + noise\n",
        "\n",
        "        # Get predictions\n",
        "        y_pred_noisy = np.argmax(model.predict(X_test_noisy), axis=1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        noisy_accuracy = accuracy_score(y_true, y_pred_noisy)\n",
        "        print(f\"  Accuracy with noise: {noisy_accuracy:.4f}\")\n",
        "        print(f\"  Accuracy drop: {baseline_accuracy - noisy_accuracy:.4f}\")\n",
        "\n",
        "        # Store results\n",
        "        accuracy_results.append({\n",
        "            \"noise_level\": noise_level,\n",
        "            \"accuracy\": float(noisy_accuracy),\n",
        "            \"accuracy_drop\": float(baseline_accuracy - noisy_accuracy)\n",
        "        })\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    noise_levels_str = [str(nl) for nl in noise_levels]\n",
        "    accuracies = [res[\"accuracy\"] for res in accuracy_results]\n",
        "\n",
        "    plt.bar(noise_levels_str, accuracies)\n",
        "    plt.axhline(y=baseline_accuracy, color='r', linestyle='--', label=f'Baseline ({baseline_accuracy:.4f})')\n",
        "    plt.xlabel('Noise Level (σ)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Model Accuracy Under Different Noise Levels')\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/noise_resilience.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save results\n",
        "    noise_results = {\n",
        "        \"baseline_accuracy\": float(baseline_accuracy),\n",
        "        \"noise_tests\": accuracy_results\n",
        "    }\n",
        "\n",
        "    return noise_results"
      ],
      "metadata": {
        "id": "sCZICXPrHfx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_adversarial_testing(X_test, y_test, model):\n",
        "    \"\"\"Perform simple adversarial testing using Fast Gradient Sign Method\"\"\"\n",
        "    print(\"\\n========== ADVERSARIAL TESTING ==========\")\n",
        "\n",
        "    # Define epsilon values to test\n",
        "    epsilons = [0.01, 0.05, 0.1, 0.2]\n",
        "\n",
        "    # Get baseline accuracy\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    baseline_accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Baseline accuracy: {baseline_accuracy:.4f}\")\n",
        "\n",
        "    # Store results for each epsilon\n",
        "    accuracy_results = []\n",
        "\n",
        "    # Create TensorFlow inputs for FGSM\n",
        "    X_tensor = tf.convert_to_tensor(X_test[:100])  # Use subset for speed\n",
        "    y_tensor = tf.convert_to_tensor(y_test[:100])\n",
        "\n",
        "    # Test each epsilon\n",
        "    for epsilon in epsilons:\n",
        "        print(f\"Testing epsilon: {epsilon}\")\n",
        "\n",
        "        try:\n",
        "            # Simple FGSM implementation\n",
        "            with tf.GradientTape() as tape:\n",
        "                tape.watch(X_tensor)\n",
        "                prediction = model(X_tensor)\n",
        "                loss = tf.keras.losses.categorical_crossentropy(y_tensor, prediction)\n",
        "\n",
        "            # Get the gradients\n",
        "            gradient = tape.gradient(loss, X_tensor)\n",
        "\n",
        "            # Create adversarial examples\n",
        "            signed_grad = tf.sign(gradient)\n",
        "            adversarial_data = X_tensor + epsilon * signed_grad\n",
        "            adversarial_data = tf.clip_by_value(adversarial_data, -1, 1)\n",
        "\n",
        "            # Get predictions on adversarial examples\n",
        "            adv_predictions = model.predict(adversarial_data)\n",
        "            adv_y_pred = np.argmax(adv_predictions, axis=1)\n",
        "            adv_y_true = np.argmax(y_test[:100], axis=1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            adv_accuracy = accuracy_score(adv_y_true, adv_y_pred)\n",
        "            print(f\"  Adversarial accuracy: {adv_accuracy:.4f}\")\n",
        "            print(f\"  Accuracy drop: {baseline_accuracy - adv_accuracy:.4f}\")\n",
        "\n",
        "            # Store results\n",
        "            accuracy_results.append({\n",
        "                \"epsilon\": epsilon,\n",
        "                \"accuracy\": float(adv_accuracy),\n",
        "                \"accuracy_drop\": float(baseline_accuracy - adv_accuracy)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error performing FGSM with epsilon={epsilon}: {str(e)}\")\n",
        "            accuracy_results.append({\n",
        "                \"epsilon\": epsilon,\n",
        "                \"accuracy\": None,\n",
        "                \"accuracy_drop\": None,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "    # Plot results (if we have any successful tests)\n",
        "    successful_tests = [r for r in accuracy_results if r[\"accuracy\"] is not None]\n",
        "    if successful_tests:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        epsilons_str = [str(r[\"epsilon\"]) for r in successful_tests]\n",
        "        accuracies = [r[\"accuracy\"] for r in successful_tests]\n",
        "\n",
        "        plt.bar(epsilons_str, accuracies)\n",
        "        plt.axhline(y=baseline_accuracy, color='r', linestyle='--', label=f'Baseline ({baseline_accuracy:.4f})')\n",
        "        plt.xlabel('Epsilon')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Model Accuracy Under Adversarial Attacks (FGSM)')\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"{RESULTS_DIR}/adversarial_testing.png\")\n",
        "        plt.close()\n",
        "\n",
        "    # Save results\n",
        "    adversarial_results = {\n",
        "        \"baseline_accuracy\": float(baseline_accuracy),\n",
        "        \"adversarial_tests\": accuracy_results\n",
        "    }\n",
        "\n",
        "    return adversarial_results"
      ],
      "metadata": {
        "id": "_W1_oD70Higu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_time_series_shift(X_test, y_test, model):\n",
        "    \"\"\"Test model performance with shifts in time series data\"\"\"\n",
        "    print(\"\\n========== TIME SERIES SHIFT TESTING ==========\")\n",
        "\n",
        "    # Define shift amounts to test (as percentage of sequence length)\n",
        "    shift_percentages = [0.05, 0.1, 0.2, 0.5]\n",
        "\n",
        "    # Get sequence length\n",
        "    sequence_length = X_test.shape[1]\n",
        "\n",
        "    # Get baseline accuracy\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    baseline_accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Baseline accuracy (no shift): {baseline_accuracy:.4f}\")\n",
        "\n",
        "    # Store results for each shift\n",
        "    shift_results = []\n",
        "\n",
        "    # Test each shift percentage\n",
        "    for shift_pct in shift_percentages:\n",
        "        shift_amount = int(sequence_length * shift_pct)\n",
        "        print(f\"Testing shift of {shift_amount} points ({shift_pct*100:.1f}% of sequence)\")\n",
        "\n",
        "        # Create left-shifted data (move signal earlier)\n",
        "        X_test_left_shifted = np.zeros_like(X_test)\n",
        "        X_test_left_shifted[:, :-shift_amount, :] = X_test[:, shift_amount:, :]\n",
        "\n",
        "        # Create right-shifted data (move signal later)\n",
        "        X_test_right_shifted = np.zeros_like(X_test)\n",
        "        X_test_right_shifted[:, shift_amount:, :] = X_test[:, :-shift_amount, :]\n",
        "\n",
        "        # Get predictions for left-shifted data\n",
        "        left_y_pred = np.argmax(model.predict(X_test_left_shifted), axis=1)\n",
        "        left_accuracy = accuracy_score(y_true, left_y_pred)\n",
        "        print(f\"  Left-shift accuracy: {left_accuracy:.4f}\")\n",
        "        print(f\"  Left-shift accuracy drop: {baseline_accuracy - left_accuracy:.4f}\")\n",
        "\n",
        "        # Get predictions for right-shifted data\n",
        "        right_y_pred = np.argmax(model.predict(X_test_right_shifted), axis=1)\n",
        "        right_accuracy = accuracy_score(y_true, right_y_pred)\n",
        "        print(f\"  Right-shift accuracy: {right_accuracy:.4f}\")\n",
        "        print(f\"  Right-shift accuracy drop: {baseline_accuracy - right_accuracy:.4f}\")\n",
        "\n",
        "        # Store results\n",
        "        shift_results.append({\n",
        "            \"shift_percentage\": shift_pct,\n",
        "            \"shift_points\": shift_amount,\n",
        "            \"left_shift\": {\n",
        "                \"accuracy\": float(left_accuracy),\n",
        "                \"accuracy_drop\": float(baseline_accuracy - left_accuracy)\n",
        "            },\n",
        "            \"right_shift\": {\n",
        "                \"accuracy\": float(right_accuracy),\n",
        "                \"accuracy_drop\": float(baseline_accuracy - right_accuracy)\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Set up axis and labels\n",
        "    shift_labels = [f\"{r['shift_percentage']*100:.1f}%\" for r in shift_results]\n",
        "    left_accuracies = [r[\"left_shift\"][\"accuracy\"] for r in shift_results]\n",
        "    right_accuracies = [r[\"right_shift\"][\"accuracy\"] for r in shift_results]\n",
        "\n",
        "    x = np.arange(len(shift_labels))\n",
        "    width = 0.35\n",
        "\n",
        "    # Create grouped bar chart\n",
        "    plt.bar(x - width/2, left_accuracies, width, label='Left Shift')\n",
        "    plt.bar(x + width/2, right_accuracies, width, label='Right Shift')\n",
        "    plt.axhline(y=baseline_accuracy, color='r', linestyle='--', label=f'Baseline ({baseline_accuracy:.4f})')\n",
        "\n",
        "    plt.xlabel('Shift Amount (% of sequence)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Model Accuracy Under Time Series Shifts')\n",
        "    plt.xticks(x, shift_labels)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/time_series_shift.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Save results\n",
        "    time_shift_results = {\n",
        "        \"baseline_accuracy\": float(baseline_accuracy),\n",
        "        \"shift_tests\": shift_results\n",
        "    }\n",
        "\n",
        "    return time_shift_results"
      ],
      "metadata": {
        "id": "x2jsWTBlAxt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= MAIN FUNCTION =================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run all tests\"\"\"\n",
        "    print(\"================= DAS MODEL TESTING SUITE =================\")\n",
        "    print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
        "\n",
        "    # Load data and model\n",
        "    X_train, y_train, X_test, y_test, model = load_data_and_model()\n",
        "\n",
        "    # Run data validation tests\n",
        "    integrity_report = check_dataset_integrity(X_train, y_train, X_test, y_test)\n",
        "    distribution_report = analyze_class_distribution(y_train, y_test)\n",
        "    unknown_report = simulate_unknown_class(X_test, model)\n",
        "\n",
        "    # Run model performance tests\n",
        "    cm_results = analyze_confusion_matrix(X_test, y_test, model)\n",
        "    metrics_results = evaluate_metrics(X_test, y_test, model)\n",
        "    cv_results = perform_cross_validation(X_train, y_train, model)\n",
        "\n",
        "    # Run robustness and edge case tests\n",
        "    noise_results = test_noise_resilience(X_test, y_test, model)\n",
        "    adversarial_results = perform_adversarial_testing(X_test, y_test, model)\n",
        "    shift_results = test_time_series_shift(X_test, y_test, model)\n",
        "\n",
        "    # Compile final report\n",
        "    final_report = {\n",
        "        \"test_timestamp\": timestamp,\n",
        "        \"data_validation\": {\n",
        "            \"integrity\": integrity_report,\n",
        "            \"distribution\": distribution_report,\n",
        "            \"unknown_class\": unknown_report\n",
        "        },\n",
        "        \"model_performance\": {\n",
        "            \"confusion_matrix\": cm_results,\n",
        "            \"metrics\": metrics_results,\n",
        "            \"cross_validation\": cv_results\n",
        "        },\n",
        "        \"robustness\": {\n",
        "            \"noise_resilience\": noise_results,\n",
        "            \"adversarial_testing\": adversarial_results,\n",
        "            \"time_series_shift\": shift_results\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save final report as JSON\n",
        "    with open(f\"{RESULTS_DIR}/final_report.json\", 'w') as f:\n",
        "        json.dump(final_report, f, indent=4)\n",
        "\n",
        "    print(f\"\\n================= TESTING COMPLETE =================\")\n",
        "    print(f\"All results saved to: {RESULTS_DIR}\")\n",
        "    print(f\"Final report: {RESULTS_DIR}/final_report.json\")"
      ],
      "metadata": {
        "id": "iLso0ZphBHDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Add missing import\n",
        "    import json\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcUHCQxYBLPy",
        "outputId": "608a86ab-0041-4442-d926-055384086540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= DAS MODEL TESTING SUITE =================\n",
            "Results will be saved to: /content/drive/MyDrive/das_testing_results_20250304_082741\n",
            "Loading training data...\n",
            "Loading test data...\n",
            "Downsampling data...\n",
            "Downsampled X_train shape: (64000, 5000, 1)\n",
            "y_train shape: (64000, 4)\n",
            "Downsampled X_test shape: (16000, 5000, 1)\n",
            "y_test shape: (16000, 4)\n",
            "Loading data and model...\n",
            "Original X_train shape: (64000, 25000)\n",
            "Original y_train shape: (64000, 4)\n",
            "Original X_test shape: (16000, 25000)\n",
            "Original y_test shape: (16000, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 50 variables whereas the saved optimizer has 98 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== DATASET INTEGRITY CHECK ==========\n",
            "NaN values in X_train: 0\n",
            "NaN values in y_train: 0\n",
            "NaN values in X_test: 0\n",
            "NaN values in y_test: 0\n",
            "Infinity values in X_train: 0\n",
            "Infinity values in X_test: 0\n",
            "No duplicate rows found in the sampled subset of training data\n",
            "X_train min: -18.643932342529297, max: 18.516048431396484\n",
            "X_test min: -18.582998275756836, max: 18.560361862182617\n",
            "\n",
            "========== CLASS DISTRIBUTION ANALYSIS ==========\n",
            "Training set class distribution:\n",
            "Bike Throttle: 16000 samples (25.00%)\n",
            "Jackhammer: 16000 samples (25.00%)\n",
            "Jumping: 16000 samples (25.00%)\n",
            "Walking: 16000 samples (25.00%)\n",
            "\n",
            "Test set class distribution:\n",
            "Bike Throttle: 4000 samples (25.00%)\n",
            "Jackhammer: 4000 samples (25.00%)\n",
            "Jumping: 4000 samples (25.00%)\n",
            "Walking: 4000 samples (25.00%)\n",
            "\n",
            "Training set imbalance: 0.00%\n",
            "Test set imbalance: 0.00%\n",
            "\n",
            "========== UNKNOWN CLASS SIMULATION ==========\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 224ms/step\n",
            "Generated 200 unknown signals\n",
            "Signals classified as 'unknown' (confidence < 0.7): 200 (100.00%)\n",
            "\n",
            "========== CONFUSION MATRIX ANALYSIS ==========\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 97ms/step\n",
            "Confusion Matrix:\n",
            "[[3625    1  363   11]\n",
            " [   0 3945   36   19]\n",
            " [   7   19 3972    2]\n",
            " [   0   26   60 3914]]\n",
            "\n",
            "Overall Accuracy: 0.9660\n",
            "\n",
            "Class: Bike Throttle\n",
            "Precision: 0.9981\n",
            "Recall: 0.9062\n",
            "F1 Score: 0.9499\n",
            "\n",
            "Class: Jackhammer\n",
            "Precision: 0.9885\n",
            "Recall: 0.9862\n",
            "F1 Score: 0.9874\n",
            "\n",
            "Class: Jumping\n",
            "Precision: 0.8964\n",
            "Recall: 0.9930\n",
            "F1 Score: 0.9422\n",
            "\n",
            "Class: Walking\n",
            "Precision: 0.9919\n",
            "Recall: 0.9785\n",
            "F1 Score: 0.9851\n",
            "\n",
            "========== METRICS EVALUATION ==========\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 98ms/step\n",
            "ROC AUC for Bike Throttle: 0.9984\n",
            "ROC AUC for Jackhammer: 0.9992\n",
            "ROC AUC for Jumping: 0.9963\n",
            "ROC AUC for Walking: 0.9996\n",
            "Macro-averaged ROC AUC: 0.9984\n",
            "\n",
            "========== CROSS VALIDATION ==========\n",
            "Performing 5-fold cross-validation on 10000 samples...\n",
            "Fold 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step\n",
            "  Accuracy: 0.9685\n",
            "Fold 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 83ms/step\n",
            "  Accuracy: 0.9635\n",
            "Fold 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step\n",
            "  Accuracy: 0.9710\n",
            "Fold 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 122ms/step\n",
            "  Accuracy: 0.9640\n",
            "Fold 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step\n",
            "  Accuracy: 0.9675\n",
            "\n",
            "Cross-validation results:\n",
            "Mean Accuracy: 0.9669 ± 0.0028\n",
            "Mean Precision: 0.9703 ± 0.0018\n",
            "Mean Recall: 0.9665 ± 0.0033\n",
            "Mean F1 Score: 0.9671 ± 0.0028\n",
            "\n",
            "========== NOISE RESILIENCE TESTING ==========\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 99ms/step\n",
            "Baseline accuracy (no noise): 0.9660\n",
            "Testing noise level: 0.01\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 102ms/step\n",
            "  Accuracy with noise: 0.9660\n",
            "  Accuracy drop: 0.0000\n",
            "Testing noise level: 0.05\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 97ms/step\n",
            "  Accuracy with noise: 0.9658\n",
            "  Accuracy drop: 0.0002\n",
            "Testing noise level: 0.1\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 98ms/step\n",
            "  Accuracy with noise: 0.9657\n",
            "  Accuracy drop: 0.0002\n",
            "Testing noise level: 0.2\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 97ms/step\n",
            "  Accuracy with noise: 0.9647\n",
            "  Accuracy drop: 0.0012\n",
            "Testing noise level: 0.5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 97ms/step\n",
            "  Accuracy with noise: 0.9623\n",
            "  Accuracy drop: 0.0037\n",
            "\n",
            "========== ADVERSARIAL TESTING ==========\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 97ms/step\n",
            "Baseline accuracy: 0.9660\n",
            "Testing epsilon: 0.01\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "  Adversarial accuracy: 0.2000\n",
            "  Accuracy drop: 0.7660\n",
            "Testing epsilon: 0.05\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334ms/step\n",
            "  Adversarial accuracy: 0.2000\n",
            "  Accuracy drop: 0.7660\n",
            "Testing epsilon: 0.1\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "  Adversarial accuracy: 0.2000\n",
            "  Accuracy drop: 0.7660\n",
            "Testing epsilon: 0.2\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "  Adversarial accuracy: 0.2000\n",
            "  Accuracy drop: 0.7660\n",
            "\n",
            "========== TIME SERIES SHIFT TESTING ==========\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 98ms/step\n",
            "Baseline accuracy (no shift): 0.9660\n",
            "Testing shift of 250 points (5.0% of sequence)\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 97ms/step\n",
            "  Left-shift accuracy: 0.3394\n",
            "  Left-shift accuracy drop: 0.6266\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 97ms/step\n",
            "  Right-shift accuracy: 0.3696\n",
            "  Right-shift accuracy drop: 0.5964\n",
            "Testing shift of 500 points (10.0% of sequence)\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 98ms/step\n",
            "  Left-shift accuracy: 0.2500\n",
            "  Left-shift accuracy drop: 0.7160\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 98ms/step\n",
            "  Right-shift accuracy: 0.3685\n",
            "  Right-shift accuracy drop: 0.5975\n",
            "Testing shift of 1000 points (20.0% of sequence)\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 106ms/step\n",
            "  Left-shift accuracy: 0.2500\n",
            "  Left-shift accuracy drop: 0.7160\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 100ms/step\n",
            "  Right-shift accuracy: 0.7245\n",
            "  Right-shift accuracy drop: 0.2415\n",
            "Testing shift of 2500 points (50.0% of sequence)\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 122ms/step\n",
            "  Left-shift accuracy: 0.2500\n",
            "  Left-shift accuracy drop: 0.7160\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 110ms/step\n",
            "  Right-shift accuracy: 0.3677\n",
            "  Right-shift accuracy drop: 0.5983\n",
            "\n",
            "================= TESTING COMPLETE =================\n",
            "All results saved to: /content/drive/MyDrive/das_testing_results_20250304_082741\n",
            "Final report: /content/drive/MyDrive/das_testing_results_20250304_082741/final_report.json\n"
          ]
        }
      ]
    }
  ]
}