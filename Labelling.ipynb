{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "80k5-MTJSQq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f16f313-d408-4779-8e64-b0f0f7fcba43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/BikeThrottle_A.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/BikeThrottle_B.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jackhammer_B.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jackhammer_B.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jumping_A.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jumping_B.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Walking_A.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Walking_B.h5\"\n",
        "]"
      ],
      "metadata": {
        "id": "LQ05FJsKU8Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def process_h5_files(file_paths):\n",
        "    \"\"\"\n",
        "    Reads multiple H5 files, assigns labels based on file names, and saves the labeled dataset.\n",
        "\n",
        "    Parameters:\n",
        "        file_paths (list): List of H5 file paths.\n",
        "\n",
        "    Returns:\n",
        "        final_data (numpy.ndarray): Combined dataset from all files.\n",
        "        final_labels (numpy.ndarray): Combined labels for the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define category-to-label mapping based on filenames\n",
        "    label_mapping = {\n",
        "        \"bikethrottlea\": 0,\n",
        "        \"bikethrottleb\": 1,\n",
        "        \"jackhammera\": 2,\n",
        "        \"jackhammerb\": 3,\n",
        "        \"jumpinga\": 4,\n",
        "        \"jumpingb\": 5,\n",
        "        \"walkinga\": 6,\n",
        "        \"walkingb\": 7\n",
        "    }\n",
        "\n",
        "    data_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Extract and normalize filename\n",
        "        file_name = file_path.split(\"/\")[-1].replace(\".h5\", \"\").replace(\"_\", \"\").lower()\n",
        "\n",
        "        # Check if the filename exists in the mapping\n",
        "        if file_name not in label_mapping:\n",
        "            print(f\"Warning: File '{file_path}' does not match any known category labels. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        label = label_mapping[file_name]\n",
        "        print(f\"Processing File: {file_path}, Assigned Label: {label}\")\n",
        "\n",
        "        # Load data from H5 file\n",
        "        with h5py.File(file_path, \"r\") as h5f:\n",
        "            dataset_name = list(h5f.keys())[0]  # Get first dataset name dynamically\n",
        "            data = np.array(h5f[dataset_name])\n",
        "\n",
        "        # Create label array\n",
        "        labels = np.full((data.shape[0],), label)\n",
        "\n",
        "        # Append data and labels\n",
        "        data_list.append(data)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    # Ensure valid data was collected\n",
        "    if not data_list or not labels_list:\n",
        "        raise ValueError(\"No valid files were processed. Please check the file names and mapping.\")\n",
        "\n",
        "    # Combine into single numpy arrays\n",
        "    final_data = np.concatenate(data_list, axis=0)\n",
        "    final_labels = np.concatenate(labels_list, axis=0)\n",
        "\n",
        "    # Save dataset\n",
        "    np.save(\"/content/drive/MyDrive/final_labeled_dataset.npy\", final_data)\n",
        "    np.save(\"/content/drive/MyDrive/final_labels.npy\", final_labels)\n",
        "\n",
        "    print(\"Final dataset and labels saved successfully!\")\n",
        "\n",
        "    return final_data, final_labels\n",
        "\n",
        "\n",
        "# ** Example Usage with Your 8 Files **\n",
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/BikeThrottle_A.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/BikeThrottle_B.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jackhammer_A.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jackhammer_B.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jumping_A.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jumping_B.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Walking_A.h5\",\n",
        "    \"/content/drive/MyDrive/IITM/IITM_fiber_DATAset/Walking_B.h5\"\n",
        "]\n",
        "\n",
        "data, labels = process_h5_files(file_paths)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV4-1RuMVBht",
        "outputId": "5478903d-f3ff-41a6-e367-7c9e7570836b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/BikeThrottle_A.h5, Assigned Label: 0\n",
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/BikeThrottle_B.h5, Assigned Label: 1\n",
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jackhammer_A.h5, Assigned Label: 2\n",
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jackhammer_B.h5, Assigned Label: 3\n",
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jumping_A.h5, Assigned Label: 4\n",
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/Jumping_B.h5, Assigned Label: 5\n",
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/Walking_A.h5, Assigned Label: 6\n",
            "Processing File: /content/drive/MyDrive/IITM/IITM_fiber_DATAset/Walking_B.h5, Assigned Label: 7\n",
            "Final dataset and labels saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking"
      ],
      "metadata": {
        "id": "WrPNGnCrbXta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "X = np.load(\"/content/drive/MyDrive/Z-Score.npy\")\n",
        "y = np.load(\"/content/drive/MyDrive/final_labels_fixed.npy\")\n",
        "\n",
        "# Check shapes\n",
        "print(f\"Shape of X (features): {X.shape}\")  # Expected: (num_samples, timesteps)\n",
        "print(f\"Shape of y (labels): {y.shape}\")  # Expected: (num_samples,)\n",
        "\n",
        "# Check if the number of samples matches\n",
        "if X.shape[0] != y.shape[0]:\n",
        "    print(f\"❌ ERROR: X has {X.shape[0]} samples, but y has {y.shape[0]} labels.\")\n",
        "else:\n",
        "    print(\"✅ Shapes are correct!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qlWNLoVbXde",
        "outputId": "3e459594-3df7-48ef-a1d6-e4ae957bd449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (features): (100, 20000000)\n",
            "Shape of y (labels): (100,)\n",
            "✅ Shapes are correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the number of labels matches the number of samples in X\n",
        "num_samples = X.shape[0]  # Get number of samples in X\n",
        "new_y = np.zeros(num_samples, dtype=int)  # Create new label array\n",
        "\n",
        "# Example: Assign labels based on the filenames or your labeling logic\n",
        "for i in range(num_samples):\n",
        "    # Example logic: alternate between labels 0 and 1 for a simple case\n",
        "    new_y[i] = i % 2  # Just an example, use actual labeling strategy\n",
        "\n",
        "# Save the fixed labels\n",
        "np.save(\"/content/drive/MyDrive/final_labels_fixed.npy\", new_y)\n",
        "\n",
        "print(f\"Fixed labels: Now y has {new_y.shape[0]} samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1X5ZpS-b8RZ",
        "outputId": "f4969faf-5d82-4939-f5b7-ddf05fe298fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed labels: Now y has 100 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI7rFTtSNHCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807e4680-c01d-4b6c-a74d-8bc8e9990e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ** Load the Correct Datasets **\n",
        "X = np.load(\"/content/drive/MyDrive/Z-Score.npy\")  # Use Z-score normalized dataset\n",
        "y = np.load(\"/content/drive/MyDrive/final_labels_fixed.npy\")  # Labels\n",
        "\n",
        "# ** Reshape Data for CNN **\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)  # Add channel dimension\n",
        "\n",
        "# ** Convert labels to categorical format **\n",
        "num_classes = len(np.unique(y))\n",
        "y = keras.utils.to_categorical(y, num_classes)\n",
        "\n",
        "# ** Split Data into Training & Testing **\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ** Define CNN Model **\n",
        "model = keras.Sequential([\n",
        "    layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(X.shape[1], 1)),\n",
        "    layers.MaxPooling1D(pool_size=2),\n",
        "    layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "    layers.MaxPooling1D(pool_size=2),\n",
        "    layers.Conv1D(filters=128, kernel_size=3, activation=\"relu\"),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(num_classes, activation=\"softmax\")  # Output Layer\n",
        "])\n",
        "\n",
        "# ** Compile Model **\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# ** Train Model **\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# ** Save the Model **\n",
        "model.save(\"/content/drive/MyDrive/cnn_fiber_optic_model.h5\")\n",
        "\n",
        "print(\"Model training complete and saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "MiY4BZcvdkoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load Data from Google Drive\n",
        "# Assuming final labeled dataset is available in /content/drive/MyDrive\n",
        "X = np.load(\"/content/drive/MyDrive/final_labeled_dataset.npy\")\n",
        "y = np.load(\"/content/drive/MyDrive/final_labels_fixed.npy\")\n",
        "\n",
        "# Step 2: Preprocess the data (assuming you are using z-score normalization)\n",
        "# Example normalization (mean 0, std 1)\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# Step 3: Check data shapes\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Ensure y has the correct number of labels\n",
        "if X.shape[0] != y.shape[0]:\n",
        "    raise ValueError(\"The number of samples in X and y does not match!\")\n",
        "\n",
        "# Step 4: Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Data Generator (for batch loading)\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, y, batch_size=32):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(self.X))\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of batches per epoch\n",
        "        return int(np.floor(len(self.X) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate a batch of data\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_X = self.X[batch_indices]\n",
        "        batch_y = self.y[batch_indices]\n",
        "        return batch_X, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indices at the end of each epoch (optional)\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# Step 6: Create data generators for training and testing\n",
        "train_generator = DataGenerator(X_train, y_train, batch_size=32)\n",
        "test_generator = DataGenerator(X_test, y_test, batch_size=32)\n",
        "\n",
        "# Step 7: Define CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(8, activation='softmax')  # Assuming 8 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 8: Train the model using the generator\n",
        "history = model.fit(train_generator, epochs=20, validation_data=test_generator)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Step 10: Save the trained model to a file\n",
        "model.save(\"/content/drive/MyDrive/final_trained_model.h5\")\n",
        "print(\"Model saved successfully to '/content/drive/MyDrive/final_trained_model.h5'\")\n"
      ],
      "metadata": {
        "id": "tI935FdGdh3m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "25ca691b-9692-4a63-ed73-46bd34446c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (2000000000,)\n",
            "y shape: (100,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The number of samples in X and y does not match!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-03d38b5886cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Ensure y has the correct number of labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of samples in X and y does not match!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Step 4: Split data into training and testing sets (80% training, 20% testing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of samples in X and y does not match!"
          ]
        }
      ]
    }
  ]
}